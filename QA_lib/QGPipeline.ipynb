{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QGPipeline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOax8HZfdGbnMgI+t39z98X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSHOENSK42Li","executionInfo":{"status":"ok","timestamp":1617568512636,"user_tz":-330,"elapsed":12334,"user":{"displayName":"Atharva Lohangade","photoUrl":"","userId":"04910854774108723270"}},"outputId":"7d9310cb-30d5-4a1d-f953-4497e4c9998d"},"source":["!pip install transformers\n","!python -m nltk.downloader punkt\n","!pip install sentencepiece"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 21.1MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 52.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 49.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=1b5d84b6915af9e9662d8470e57ac86601d85b7cfbac2789aa8cf2b7c2567a1a\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.1 transformers-4.4.2\n","/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","  warn(RuntimeWarning(msg))\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 20.2MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wdrpA6lV4qyD","executionInfo":{"status":"ok","timestamp":1617568914978,"user_tz":-330,"elapsed":1400,"user":{"displayName":"Atharva Lohangade","photoUrl":"","userId":"04910854774108723270"}}},"source":["import itertools\n","\n","from nltk import sent_tokenize\n","\n","import torch\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","PIPELINE_SETTINGS = {\n","    #\"model\": \"valhalla/t5-base-qg-hl\",\n","    \"model\": \"mrm8488/t5-base-finetuned-question-generation-ap\",\n","    \"ans_model\": [\"valhalla/t5-base-qa-qg-hl\", \"valhalla/t5-small-qa-qg-hl\"]\n","}\n","\n","class QGPipeline:\n","\n","    def __init__(self, pipeline_settings: dict = PIPELINE_SETTINGS, use_cuda: bool = True) :\n","\n","        self.model = AutoModelForSeq2SeqLM.from_pretrained(pipeline_settings['model'])\n","        self.tokenizer = AutoTokenizer.from_pretrained(pipeline_settings['model'], use_fast=False)\n","\n","        self.ans_model = []\n","        self.ans_tokenizer = []\n","        for i in range(len(pipeline_settings['ans_model'])) :\n","            self.ans_model.append(AutoModelForSeq2SeqLM.from_pretrained(pipeline_settings['ans_model'][i]))\n","            self.ans_tokenizer.append(AutoTokenizer.from_pretrained(pipeline_settings['ans_model'][i], use_fast=False))\n","\n","        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n","        self.model.to(self.device)\n","\n","        for i in range(len(self.ans_model)) :\n","            if self.ans_model[i] is not self.model:\n","                self.ans_model[i].to(self.device)\n","\n","    def __call__(self, text : str):\n","        input_text = \" \".join(text.split())\n","        answers = self._extract_answers(input_text)\n","        # print(sents)\n","        # print(answers)\n","        if len(answers) == 0:\n","          return []\n","\n","        questions = self._generate_questions(answers, input_text)\n","        question_answers_list = []\n","        for question, answer in zip(questions, answers) :\n","            question_answers_list.append({'question': question, 'answer': answer})\n","        return question_answers_list\n","\n","    def _extract_answers(self, context):\n","        inputs = self._prepare_inputs_for_ans_extraction(context)\n","        inputs = self._tokenize(inputs, padding=True, truncation=True)\n","\n","        answers = []\n","        for i in range(len(self.ans_model)) :\n","            outs = self.ans_model[i].generate(\n","                input_ids=inputs['input_ids'].to(self.device), \n","                attention_mask=inputs['attention_mask'].to(self.device), \n","                max_length=32,\n","            )\n","            \n","            dec = [self.ans_tokenizer[i].decode(ids, skip_special_tokens=False) for ids in outs]\n","            decoded_output = [item.split('<sep>') for item in dec]\n","            decoded_output = [i[0] for i in decoded_output]\n","            answers.extend(decoded_output)\n","        \n","        for i in range(len(answers)) :\n","            answers[i] = answers[i].replace(\"<pad> \", \"\")\n","\n","        answers = list(set(answers))\n","        return answers\n"," \n","    def _prepare_inputs_for_ans_extraction(self, text):\n","        sents = sent_tokenize(text)\n","\n","        inputs = []\n","        for i in range(len(sents)):\n","            source_text = \"extract answers:\"\n","            for j, sent in enumerate(sents):\n","                if i == j:\n","                    sent = \"<hl> %s <hl>\" % sent\n","                source_text = \"%s %s\" % (source_text, sent)\n","                source_text = source_text.strip()\n","            \n","            source_text = source_text + \" </s>\"\n","            inputs.append(source_text)\n","\n","        return inputs\n","  \n","    def _tokenize(self, inputs, padding=True, truncation=True, add_special_tokens=True, max_length=512):\n","        inputs = self.ans_tokenizer[0].batch_encode_plus(\n","            inputs, \n","            max_length=max_length,\n","            add_special_tokens=add_special_tokens,\n","            truncation=truncation,\n","            padding=\"max_length\" if padding else False,\n","            pad_to_max_length=padding,\n","            return_tensors=\"pt\"\n","        )\n","        return inputs\n","    \n","    def _generate_questions(self, answers, context):\n","        questions = []\n","        for answer in answers :\n","            input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n","            inputs = self._tokenize([input_text], padding=True, truncation=True)\n","        \n","            outs = self.model.generate(\n","                input_ids=inputs['input_ids'].to(self.device), \n","                attention_mask=inputs['attention_mask'].to(self.device), \n","                max_length=64,\n","                num_beams=4,\n","            )\n","            questions.extend([self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs])\n","\n","        for i in range(len(questions)) :\n","            questions[i] = questions[i].replace(\"question: \", \"\")\n","\n","        return questions"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJmsP07b5Bq5","executionInfo":{"status":"ok","timestamp":1617568936432,"user_tz":-330,"elapsed":21017,"user":{"displayName":"Atharva Lohangade","photoUrl":"","userId":"04910854774108723270"}},"outputId":"b947da21-6a8d-455c-8012-2baa8021adfa"},"source":["text2 = \" (1) The Internal Committee or, as the case may be, the Local Committee, may, \\\n","before initiating an inquiry under section 11 and at the request of the aggrieved woman take steps to settle \\\n","the matter between her and the respondent through conciliation: \\\n","Provided that no monetary settlement shall be made as a basis of conciliation. \\\n","(2) Where settlement has been arrived at under sub-section (1), the Internal Committee or the Local \\\n","Committee, as the case may be, shall record the settlement so arrived and forward the same to the \\\n","employer or the District Officer to take action as specified in the recommendation. \\\n","(3) The Internal Committee or the Local Committee, as the case may be, shall provide the copies of \\\n","the settlement as recorded under sub-section (2) to the aggrieved woman and the respondent. \\\n","(4) Where a settlement is arrived at under sub-section (1), no further inquiry shall be conducted by the \\\n","Internal Committee or the Local Committee, as the case may be. \"\n","\n","qg = QGPipeline()\n","qg(text2)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:175: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n","  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[{'answer': 'monetary settlement ',\n","  'question': 'What shall not be made as a basis of conciliation?'},\n"," {'answer': 'no further inquiry ',\n","  'question': 'What happens if a settlement is reached under sub-section (1)?'},\n"," {'answer': 'record the settlement so arrived and forward the same to the employer or the District Officer ',\n","  'question': 'Where a settlement has been reached under sub-section (1), the Internal Committee or the Local Committee shall do what?'},\n"," {'answer': 'the employer or the District Officer ',\n","  'question': 'Who shall the settlement be sent to?'},\n"," {'answer': 'The Internal Committee or the Local Committee ',\n","  'question': 'Who may take steps to settle the matter between the aggrieved woman and the respondent?'}]"]},"metadata":{"tags":[]},"execution_count":9}]}]}